# ğŸ¬ Emotion-Based Movie Recommender

> **âœ¨ This project explores the trade-off between optimizing for accuracy (past behavior) and enabling human discovery (future growth).**
> 
> - Optimizing for clicks and past behavior can limit discovery  
> - Recommenders risk becoming echo chambers of taste  
> - We shift from predicting behavior to supporting emotional growth  
> - This tool helps people **name and explore** feelings â€” not just repeat them  
> 
> ğŸ’¡ *Can we build a system that helps people say:*  
> â€œ**Thatâ€™s exactly how I needed to feel â€” and I didnâ€™t even know it.**â€

---

## ğŸ” Problem

Traditional recommender systems often suggest movies by:
- Genre (e.g. "comedy", "thriller")
- User behavior (e.g. watch history, ratings)

But:
- **Genre â‰  emotional experience** â€” people crave specific emotional textures like "quiet intimacy" or "bittersweet closure"
- **Behavior â‰  true preference** â€” what users have watched may reflect social norms, not authentic taste

This project aims to recommend movies based on **emotional resonance**, not just past behavior.

---

## ğŸ§  Methodology

### 1. **Emotional Tagging with LLM**
We used the OpenAI API to tag ~5,000 TMDb movies using four emotional dimensions:

```json
{
  "title": "Movie Title",
  "emotional_experience": ["deep grief", "hopeful release"],
  "relational_themes": ["estranged siblings"],
  "life_moments": ["coming to terms with loss"],
  "aesthetic": ["muted tones", "slow pacing"]
}
```

### 2. **Embedding & Similarity Matching**
- Transformed tags into vector embeddings
- Stored movie vectors locally in a pickle-based database
- Compared user input embedding using **cosine similarity**
- Returned top 3 emotionally aligned movies

### 3. **User Interface**
- Lightweight app (e.g. Streamlit or CLI)
- Users describe the emotion they want to feel
- System recommends emotionally matched films

---

## ğŸ’¡ Why Cosine Similarity?
Cosine similarity compares **angle** (not length) between vectors â€” perfect for capturing **semantic alignment** in text-based embeddings.  
Other distances (e.g. Euclidean) can distort meaning by over-weighting magnitude, especially in high-dimensional space.

---

## ğŸ“Š Evaluation (or Lack Thereof)

No traditional evaluation (e.g. Precision@K) was performed because:
- Emotional fit is **subjective**
- Standard metrics require labeled ground truth, which we donâ€™t have

### Future Evaluation Ideas:
- Collect **thumbs up/down** feedback from users
- Ask: *â€œDid this match how you wanted to feel?â€*
- Track engagement over time and refine tag quality

---

## ğŸ”„ Lessons & Next Steps

### Optimizing vs. Exploring
- Traditional recommenders optimize for past behavior â€” but risk limiting discovery
- Our goal: build tools that help users **name and explore** emotional needs, not just reinforce them

> ğŸ’¬ *â€œSuccess isnâ€™t just relevance â€” itâ€™s resonance.â€*

### Next Steps
- Add user feedback loop
- Improve emotional tag consistency with multi-rater checks or LLM validation
- Let users **tag themselves emotionally** to personalize recommendations

---

## ğŸ“ Project Structure

```
emotion_recommender/
â”œâ”€â”€ data/                     # Raw and processed TMDb data
â”œâ”€â”€ tagging/                  # LLM API tagging scripts
â”œâ”€â”€ embedding/                # Embedding generation + similarity matching
â”œâ”€â”€ app/                      # User-facing app (e.g. Streamlit)
â”œâ”€â”€ utils/                    # Helper functions
â””â”€â”€ README.md                 # This file
```

---

## ğŸ¤ Acknowledgements

Inspired by conversations about taste, art, and emotional truth â€” thank you to friends who challenged the algorithmic way of seeing.

---

## ğŸ“¬ Contact

Feel free to reach out with questions, suggestions, or emotional movie recs!  
[Your Name] â€“ [your.email@example.com] â€“ [LinkedIn or GitHub link]
